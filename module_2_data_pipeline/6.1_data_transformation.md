# Data Transformation

Data transformation is the step in the ETL (Extract, Transform, Load) process where extracted data is cleaned, adjusted, and formatted according to business rules, improving its quality for analysis and integration.

## Main Subprocesses of Transformation:

- **Data Cleaning**: Identifying and correcting errors, inconsistencies, and missing values.
- **Data Normalization**: Standardizing units, dates, and currencies.
- **Data Aggregation**: Summarizing data at higher levels of granularity (e.g., from daily to monthly).
- **Data Filtering**: Removing irrelevant data, keeping only what's necessary.
- **Data Concatenation**: Combining datasets from different sources.
- **Data Derivation**: Creating new data from calculations or transformations (e.g., indexes or variables).
- **Data Mapping**: Matching values between different sources.
- **Data Standardization**: Ensuring data follows conventions for easier integration between systems.
- **Data Enrichment**: Adding external information (e.g., customer demographic data).
- **Aggregation of Unstructured Data**: Transforming unstructured data (e.g., text, images) into structured formats.
- **Dimensionality Reduction**: Reducing variables while retaining relevant information (used in machine learning).
- **Outlier Removal**: Identifying and removing outliers that distort analyses.
- **Data Segmentation**: Dividing data into groups based on specific criteria.
- **Categorical Data Encoding**: Converting categorical variables into numerical formats for machine learning.

Data transformation is essential to ensure that data is ready for analysis, integration, and decision-making.

---

# Types of Data

Identifying the types of data present in a dataset, such as integers, floats, dates, strings, among others, is crucial.

## Qualitative (Categorical) Variables
These are characteristics defined by categories, without quantitative values, and can be:

- **Nominal**: No order between categories.  
  Examples: gender, eye color, smoker/non-smoker.

- **Ordinal**: There is an order between categories.  
  Examples: education level, disease stage, month of observation.

## Quantitative Variables
These are characteristics that can be measured on a numeric scale and can be:

- **Discrete**: Take a finite or countable number of integer values, usually resulting from counts.  
  Examples: number of children, number of cigarettes smoked per day.

- **Continuous**: Take values on a continuous scale, allowing fractional values.  
  Examples: weight, height, time, blood pressure.

---

# Data Structure

The data structure can vary based on the format in which it is organized, such as tabular, hierarchical, or others.

## Types of Data Formats

- **Text (CSV)**: Data stored as plain text, with fields separated by commas.
- **JSON**: Lightweight data exchange format with a key-value pair structure, used in APIs and web applications.
- **XML**: Markup format with a hierarchical structure using tags.
- **Relational Database (SQL)**: Data stored in tables with relationships, managed by SQL.
- **NoSQL Database**: Formats such as documents (MongoDB), columns (Cassandra), graphs (Neo4j), and key-value pairs (Redis).
- **Binary Files**: Data stored in binary format, such as images (JPEG), audio (MP3), and video (MP4).
- **Parquet and Avro**: Formats optimized for Big Data analysis in ecosystems like Hadoop and Spark.
- **HDF5**: Format used for large scientific and multidimensional datasets.
- **ARFF**: Format for datasets used in machine learning.
- **Log Format**: Storing system events (e.g., Apache logs, syslogs).
- **DICOM**: Format for storing medical images, such as X-rays and CT scans.
- **Geospatial Data (Shapefile, GeoJSON)**: Formats for geospatial data such as maps and locations.
- **GTFS**: Format for public transportation data, organized in CSV files with information about agencies, routes, stops, and schedules.

---

# Data Cleaning and Organization

Data cleaning and organization is a critical process to ensure data is ready for analysis, processing, or use in applications. It involves detecting, correcting, and organizing data to eliminate errors and inconsistencies.

## Key Steps in the Process

- **Detection of Missing Data**: Identifying and handling missing values, either by filling in estimates or excluding records.
- **Elimination of Duplicates**: Removing duplicate records to avoid skewed analyses.
- **Data Standardization**: Consistency in formats, such as dates, currencies, and units of measure.
- **Correction of Typographical Errors**: Fixing typos or formatting errors.
- **Data Validation**: Ensuring compliance with business rules or specific validations.
- **Data Normalization**: Transforming data into a comparable format across different sources.
- **Removal of Outliers**: Handling outliers that distort analyses and models.
- **Value Mapping**: Replacing coded or abbreviated values with more meaningful ones.
- **Data Aggregation**: Summarizing data at different levels of granularity (e.g., from daily to monthly).
- **Sorting and Indexing**: Organizing and creating indexes for efficient queries.
- **Data Segmentation**: Dividing data into groups based on specific criteria.
- **Normalization of Names and Entities**: Standardizing names and entities for consistency.
- **Referential Integrity Check**: Ensuring consistency across tables in relational databases.

---

# Credit Card Fraud Detection

Credit card fraud detection is crucial to protect customers and minimize financial losses. It uses various techniques to identify suspicious activities.

## Key Steps and Techniques

- **Data Collection**: Collecting transaction data, such as cardholder information, merchant details, amount, and location.
- **Historical Behavior Analysis**: Analyzing transaction history to identify normal patterns.
- **Anomaly Detection**: Algorithms detect transactions outside of normal patterns, such as unusually high amounts or atypical behavior.
- **Real-time Monitoring**: Transactions are monitored in real-time with alerts for suspicious activities.
- **Statistical Modeling and Machine Learning**: Models and machine learning techniques are used to detect subtle anomalies.
- **Integration of External Data**: Incorporating additional data, such as credit information and lists of stolen cards.
- **Rules and Heuristics**: Using predefined rules to identify suspicious patterns, such as high-risk locations or unusual spending.
- **Continuous Learning**: Systems learn from new data, improving detection accuracy.
- **Alerts and Response**: Alerts are generated for suspicious transactions, leading to actions like card blocking and investigations.
- **Manual Review**: Fraud analysts manually review suspicious transactions.
- **Customer Feedback**: Customers can report suspicious transactions to improve detection systems.
- **Security Regulations**: Complying with security regulations, such as PCI DSS, to protect credit card data.
