# **Data Extraction: Theory and Practice**

### **Difference Between Data Extraction and Data Ingestion**

- **Data Extraction**:
  - Refers to the process of **collecting data** from various sources, such as databases, files, applications, websites, and social media.
  - Involves obtaining data from external sources, which can be raw or pre-processed data.
  - Often includes an **initial transformation** to clean, format, or structure the data.
  - Typically utilizes **ETL (Extract, Transform, Load)** tools to facilitate and automate the process.

- **Data Ingestion**:
  - Refers to the process of **receiving, validating, and loading** the extracted data into a storage system (like a Data Warehouse or Data Lake).
  - Ingestion handles the **transfer and storage** of the data, without necessarily transforming it at this stage.
  - Can be performed in two ways: **batch processing** or **real-time streaming**.
  - The focus is on **bringing** the data into the storage system so it can be analyzed or processed later.

- **Key Differences**:
  - **Extraction** collects and transforms data from external sources, while **ingestion** focuses on receiving and storing that data in a centralized system.
  - Extraction may be part of the ETL process, while ingestion doesn’t involve transformation—only transferring the data for storage.
  - Extraction happens before ingestion, as an initial step in preparing data for analysis.

---

## **ETL Process (Extract, Transform, Load)**

The **ETL** (Extract, Transform, Load) process is crucial in data integration, especially for **Data Warehousing** projects. It consists of:

- **Extraction (Extract)**: Data is retrieved from different sources.
- **Transformation (Transform)**: The extracted data is transformed to ensure quality and consistency.
- **Loading (Load)**: The transformed data is loaded into a data warehouse or other target system.

While transformation is optional, it is considered a best practice to ensure clean and aligned data. As Ralph Kimball states, an efficient ETL system applies quality standards and prepares the data for analysis.

### **Common Data Sources**:
- Relational databases
- Text files (plain text)

It’s important that the ETL tool can communicate with these sources, which can be a challenging task, especially if the sources are not easily accessible.

---

## **Data Extraction**

**Data extraction** is the first step in the **ETL process** and involves collecting data from original sources (such as operational systems and external sources) and transferring it to a centralized environment, such as a **Data Warehouse (DW)**.

- **Sources of Extraction**: Data can come from various sources, such as management systems (ERP, CRM, SIG), DBMS (Oracle, SQL Server), Excel spreadsheets, and text files.
- **Objective**: The goal is to obtain raw data from different sources and move it to a temporary area, called the **staging area**, where it will be converted to a common format.

### **Extraction Steps**:
1. **Identification and Retrieval**: Data is extracted from OLTP (Online Transaction Processing) systems and moved to the central repository.
2. **Format Conversion**: Due to the heterogeneity of the data, converting it to a unified format is necessary to ensure proper handling.
3. **Technical Processes**: Extraction may involve querying databases, using APIs, web scraping, and transforming data.

### **Difference Between Collection and Extraction**:
- **Collection**: Involves the initial gathering of data from internal or external sources, which may include methods like surveys, APIs, or logs.
- **Extraction**: Focuses on moving and preparing the data for storage and subsequent analysis.

Extraction is crucial for ensuring that data is ready for analysis and future use in activities such as reporting and data processing.

---

## **Key Concepts in Data Extraction**

**Data extraction** is the process of identifying, retrieving, and moving information from original sources to a centralized location. Some key concepts include:

- **Data Sources**: Locations where data is obtained from, such as databases, applications, APIs, files, logs, and web sources.
- **Raw Data**: Unprocessed information, which can be unstructured, semi-structured, or structured.
- **ETL Extraction**: The process involving **Extraction**, **Transformation** (for cleaning and preparation), and **Loading** of data into a central repository.
- **SQL Queries**: Used to retrieve specific data from tables or databases.
- **Data Transformation**: During extraction, raw data may be cleaned, formatted, or enriched as needed.
- **Data Movement**: Transferring the extracted data to a central repository, such as a **Data Lake** or **Data Warehouse**.
- **Scheduling and Automation**: Extraction can be scheduled to occur regularly, ensuring up-to-date data.
- **Security and Privacy**: Security measures, such as encryption and authentication, are essential for protecting the data.
- **Monitoring and Logging**: Continuous monitoring of the process to identify issues, with useful logs for auditing and troubleshooting.
- **Integration with Data Flow**: The extracted data is integrated into the data flow for transformation and analysis.
- **Failure Handling**: Considerations for handling failures, such as connectivity issues or unavailability of sources.
