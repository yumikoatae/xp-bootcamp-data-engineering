# Crawling

With the growth of unstructured data in the Big Data era, **crawlers** have emerged as automated robots capable of scanning, organizing, and structuring large volumes of data. Primarily used in search engines like Google, crawlers collect, index, and store information from web pages, allowing it to be searched or used for real-time data analysis.

## Characteristics and Functions of Crawlers

- **Automated Navigation**: They automatically browse the web, following links to discover and access new pages.
- **Data Collection**: They extract information such as text, images, links, and metadata.
- **Indexing**: They organize and index data to facilitate search and retrieval.
- **Regular Updates**: They operate at regular intervals to keep the data up to date.
- **Search Engine Usage**: Crawlers track and index websites to make user searches easier.
- **Change Monitoring**: They track changes on pages over time, such as price updates or news.
- **Broken Link Checking**: They detect invalid links or missing pages.
- **Specific Information Retrieval**: They collect specific data, like product prices on e-commerce sites.
- **Adherence to Robot Guidelines**: They respect rules defined in the "robots.txt" file of each site.
- **Ethics and Privacy**: Data collection should be done ethically, respecting privacy and site guidelines.

Crawlers are essential not only for search engines but also in areas like competitive analysis and price monitoring. However, it is critical to ensure their ethical and legal use, adhering to site policies.

---

# Scraping

Scraping is the automated process of collecting information from web pages, such as websites, forums, social media, online stores, and other online resources. The goal of scraping is to extract specific data like text, images, links, product prices, customer reviews, and other information systematically.

## Purpose of Scraping

Scraping is typically used for the following purposes:

- **Data Collection for Analysis**: Used by companies and researchers to gather data for market analysis, competitive research, public opinion studies, and trend analysis.
- **Price Monitoring**: Used by online stores and consumers to track product prices and receive alerts when changes occur.
- **News Aggregation**: News platforms and blogs use scraping to collect and display articles from various sources in one place.
- **Customer Review Collection**: Businesses collect customer reviews from different sources for feedback analysis.
- **Content Generation**: Some websites use scraping to gather information and generate content automatically.
- **Academic Research**: Researchers collect data from multiple sources for academic studies.
- **Price and Product Comparison**: Consumers compare prices and features of products from different online stores.

It is important to note that scraping can be legally complex. Not all websites allow data collection, and in some cases, scraping may violate terms of service. Data protection and privacy laws also apply, making it essential to respect site policies and obtain permission when necessary. Tools like **Beautiful Soup** and **Scrapy** in Python make automating this process easier.

## Web Scraping – Methods of Doing It

There are several techniques and tools for performing web scraping, ranging from simple data copying to complex automation tools.

### Web Scraping – HTTP

Web scraping via HTTP refers to the practice of extracting information from web pages by making HTTP requests to the web servers that host these pages.

#### Overview of How Web Scraping via HTTP Works

1. **HTTP Request**: A request is sent to the URL of the web page from which data will be collected.
2. **Sending the Request**: The HTTP request is sent to the server hosting the web page.
3. **Server Response**: The server processes the request and sends back an HTTP response containing the HTML content of the page.
4. **HTML Content Parsing**: Tools like Beautiful Soup are used to parse the HTML content and extract specific information.
5. **Storage or Processing**: After extraction, the data can be stored, processed, or displayed.
6. **Iteration or Automation**: The process can be repeated for multiple pages or at regular intervals.

### Parsing in Web Scraping

**Parsing** is the process of analyzing the HTML or XML content of a web page to extract specific information and structure it.

#### Key Aspects of Parsing

- **Source Code Analysis**: After an HTTP request, the HTML or XML content of the page is analyzed to locate the desired data.
- **Selectors and Queries**: CSS selectors, XPath queries, or search methods from libraries like Beautiful Soup are used to locate the data.
- **Data Extraction**: Once the relevant elements are identified, data such as text, attributes, or links are extracted.
- **Cleaning and Formatting**: Extracted data may undergo a cleaning and formatting process.
- **Structuring the Data**: The data is organized into lists, dictionaries, or tables for easier manipulation.
- **Iteration Across Multiple Pages**: In sites with multiple pages, parsing is repeated to gather data from all of them.
- **Error Handling**: It's important to handle errors, such as missing elements or unexpected structures in the source code.

Parsing is a crucial part of web scraping as it transforms the unstructured content of pages into organized and usable data. Libraries like **Beautiful Soup**, **lxml**, and **Scrapy** offer powerful features for performing this process effectively.

### Web Scraping – JSON

Web scraping via JSON refers to the process of collecting structured data directly from APIs (Application Programming Interfaces) that return information in the JSON (JavaScript Object Notation) format. Unlike traditional web scraping, which involves analyzing the HTML or XML content of web pages, web scraping via JSON involves extracting data from API endpoints that provide information in a structured and easy-to-process format.

### Web Scraping – API

API stands for Application Programming Interface. It is a set of rules and protocols used to build software applications. In the context of web scraping, an API is a way to collect clean data from a website, meaning data that is not encapsulated in HTML, JavaScript, or tied to HTTP requests, etc.

## Best Practices for Scraping

1. **Avoid Overloading**: Do not intentionally overload the target web server with excessive requests, as this can harm the site's performance and is considered unethical.
2. **Do Not Manipulate Data**: Do not manipulate or alter data maliciously or deceptively. Maintain the integrity of the data.
3. **Do Not Share Sensitive Data**: If collecting private or sensitive data, do not share or disclose it without proper consent or legal authorization.
4. **Credit and Attribution**: If using publicly collected data, provide proper credit to the site or source of the data.
5. **Ethical Monitoring**: Perform scraping ethically, avoiding violations of site policies.
6. **Data Security**: Ensure the collected data is handled securely and protected from unauthorized access.
7. **Legitimate Purpose**: Use the collected data for legitimate and ethical purposes, such as research, analysis, or creating value.

## Identifying the API

1. **Identify the API**: First, identify the API from which you want to collect data. It could be a public or private API provided by an online service or platform.
2. **HTTP Request**: Send HTTP requests to the API endpoints using an HTTP client, such as the `requests` module in Python. These requests are typically made with HTTP methods like GET or POST, and may include parameters or headers.
3. **Processing the JSON Response**: When the API responds, it typically returns data in JSON format. You can use JSON parsing libraries like Python's `json` module to process the response and turn it into usable objects.
4. **Data Extraction**: After processing the JSON data, extract the desired information by navigating through the JSON objects, lists, and fields.
5. **Storage or Processing**: After extraction, you can store the data in a database or process it as needed for analysis or other activities.

## Web Scraping – Ethics

Web scraping can be a powerful technique for gathering data, but it must be done in an ethical and responsible manner. Respecting laws, site guidelines, and user privacy is essential.
