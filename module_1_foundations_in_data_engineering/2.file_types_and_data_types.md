# Common File Formats in Data Pipelines

## TXT (Plain Text):
- **Description:** Simple and human-readable file used to store data in plain text without a formal structure.
- **Application:** Generally used for unstructured or semi-structured data that requires extraction by position or regular expressions.
- **Advantage:** High human readability.
- **Disadvantage:** Low efficiency in terms of space and complexity in data extraction and manipulation.
- **Use Case:** Storing system logs and configuration files that record events in plain text, making it easy to read and analyze.

## CSV (Comma Separated Values):
- **Description:** Stores data in rows and columns, with values separated by commas.
- **Application:** Ideal for structured data, such as transaction tables or simple tabular data.
- **Advantage:** Compatible with various tools and easy to manipulate in Python, Pandas, SQL, among others.
- **Disadvantage:** Inefficient in storage for large volumes and lacks support for hierarchical data.
- **Use Case:** Financial reports and sales tables exported as CSV, easily integrated into analysis tools.

## JSON (JavaScript Object Notation):
- **Description:** A structured and hierarchical format that allows data at different levels of nesting.
- **Application:** Commonly used for transferring data in APIs and web services.
- **Advantage:** Flexible for semi-structured data and easy to interpret.
- **Disadvantage:** Increased complexity with nested data and less space-efficient compared to binary formats.
- **Use Case:** Communication between systems via RESTful APIs, with data on clients or products that may have different levels of detail.

## ORC (Optimized Row Columnar):
- **Description:** A binary format optimized for row and column-based storage, providing efficient data storage and reading.
- **Application:** Frequently used for large volumes of data in the Hadoop ecosystem.
- **Advantage:** High compression and efficiency for reading specific columns.
- **Disadvantage:** May require more processing for writing data.
- **Use Case:** Storing large volumes of historical data in data warehouse systems where efficient reading is essential.

## Avro:
- **Description:** A binary, structured format used with Apache Avro, a data serialization system.
- **Application:** Useful for semi-structured data and data flows where the schema needs to evolve.
- **Advantage:** Allows defining data schemas in multiple languages, with high compression.
- **Disadvantage:** Requires more configuration to define and manage schemas.
- **Use Case:** Data transmission in streaming pipelines, allowing schema evolution as needed.

## SequenceFile:
- **Description:** Stores data in key-value binary pairs, primarily used with Hadoop.
- **Application:** Used for distributed processing in Hadoop, ideal for data requiring fast access by key.
- **Advantage:** Efficient compression and native support in Hadoop.
- **Disadvantage:** Limited flexibility outside the Hadoop environment.
- **Use Case:** Storing intermediate data in Hadoop systems, such as in MapReduce workflows, where data is quickly retrieved by key.

## Parquet:
- **Description:** A columnar-oriented format optimized for efficient storage and processing.
- **Application:** Common in Big Data and analytical frameworks, like Spark and Hive.
- **Advantage:** Significant data size reduction and efficient querying by columns.
- **Disadvantage:** Requires more computational effort for writing data.
- **Use Case:** Storing and querying analytical data in Spark and Hive, focusing on fast and efficient reading of specific columns.

## Delta:
- **Description:** An evolution of Parquet, created by Databricks for Delta Lake, with versioning and logging support.
- **Application:** Ideal for data lakes where version control and log storage are desired.
- **Advantage:** ACID support, rollback, and historical data capabilities, suitable for Data Lakes and pipelines.
- **Disadvantage:** Dependent on the Delta Lake ecosystem to fully utilize its features.
- **Use Case:** Version control and data auditing in Data Lakes, with rollback support, version history, and optimized queries.

# Data Types: [Official Reference](https://spark.apache.org/docs/3.5.3/sql-ref-datatypes.html#supported-data-types)
Data types define the nature of data, such as numbers, text, or more complex structures. Spark supports various data types including primitive types (e.g., `Integer`, `String`) and complex types (e.g., `Array`, `Map`).


# Data Structure Classification

### Structured Data:
- **Definition:** Data organized in tables with rows and columns, with a rigid and defined structure. 
- **Example:** Relational databases such as MySQL and PostgreSQL.
- **Ideal Format:** CSV, Parquet, ORC (due to row and column structure).
- **Application:** SQL queries, BI, and analytics where data is well-defined and uniform.
- **Use Case:** CRM systems storing customer data in tables with defined fields, making extraction and analysis easier.

### Semi-Structured Data:
- **Definition:** Data that is partially organized, with flexibility in hierarchy and structure. 
- **Example:** JSON, XML.
- **Ideal Format:** JSON, Avro, Delta (for schema evolution and flexibility).
- **Application:** APIs, communication between systems, and systems where data structure may vary.
- **Use Case:** IoT sensor data, where different devices may send varying types of information in JSON, depending on their configurations.

### Unstructured Data:
- **Definition:** Data without a specific format, such as text, images, and videos, with no predefined organization.
- **Example:** Image files, video files, and text documents.
- **Ideal Format:** TXT, binary files, or images.
- **Application:** Natural language processing, media analysis, and big data where data does not have a defined structure.
- **Use Case:** Storing and analyzing social media texts, images, and videos, such as in natural language processing (NLP) or computer vision projects.
