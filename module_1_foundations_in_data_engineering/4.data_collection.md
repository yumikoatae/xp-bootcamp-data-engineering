# Crawler and Scraping

Crawler and Scraping are techniques used to collect information from the web in an automated way. While they are related, each has a specific purpose.

![image](https://github.com/user-attachments/assets/b8a9af20-eba4-4716-800e-2dc23bef6d18)


## What is a Crawler?

- A **Crawler** (or Web Crawler) is a program that navigates the internet automatically, following links to explore new pages.
- It acts like a "robot" that tracks websites to find data, visiting pages and gathering information about them.
- Crawlers are used by search engines, like Google, to index websites and keep their databases up-to-date.
- The main goal of a crawler is to find and identify new pages and track content updates.

**Use Case Example:** Google uses crawlers to explore the internet, collect information about new websites, and index them so they appear in search results.

## What is Scraping?

- **Scraping** (or Web Scraping) is the process of extracting specific data from web pages.
- A scraper is programmed to access a webpage and collect specific information, such as product prices, descriptions, contact data, etc.
- Unlike a crawler, which explores the web in various directions, a scraper focuses on collecting structured or specifically formatted data from a website.

**Use Case Example:** A company may use scraping to automatically gather competitor pricing information to adjust its own pricing.

## Difference between Crawler and Scraping

| Aspect           | Crawler                                             | Scraping                                          |
|------------------|-----------------------------------------------------|---------------------------------------------------|
| Function         | Navigation and discovery of new pages               | Extraction of specific data                       |
| Purpose          | Indexing and updating content                       | Collecting structured information                 |
| Use Case Examples| Search engines, like Google                         | Collecting prices, descriptions, or contact data  |


# API â€“ Application Programming Interface

An API (Application Programming Interface) is a mechanism that enables automated communication between two software components, facilitating the exchange of information and functionalities.

![image](https://github.com/user-attachments/assets/8c84d6e8-df43-4caf-9e97-bbce6c116299)

## 1. **SOAP APIs (Simple Object Access Protocol)**:
   - Uses the SOAP protocol to exchange messages in XML format between client and server.
   - This is a less flexible API that was more popular in the past.

## 2. **RPC APIs (Remote Procedure Calls)**:
   - Known as Remote Procedure Calls, they allow the client to request the server to execute a specific function and then receive the response with the result.

## 3. **WebSocket APIs**:
   - Use the WebSocket protocol for bidirectional communication between client and server.
   - Transmit data in JSON objects, allowing the server to send callback messages to connected clients, making them more efficient than REST APIs for certain applications.

## 4. **REST APIs (Representational State Transfer)**:
   - These are the most popular and flexible APIs found on the Web today.
   - The client sends requests to the server, which processes these requests and returns data as a response. REST uses the HTTP standard, making it widely compatible with various applications.


